{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tipe data text_train: <class 'list'>\n",
      "panjang data text_train: 25000\n",
      "data pertama text_train: b'Words can\\'t describe how bad this movie is. I can\\'t explain it by writing only. You have too see it for yourself to get at grip of how horrible a movie really can be. Not that I recommend you to do that. There are so many clich\\xc3\\xa9s, mistakes (and all other negative things you can imagine) here that will just make you cry. To start with the technical first, there are a LOT of mistakes regarding the airplane. I won\\'t list them here, but just mention the coloring of the plane. They didn\\'t even manage to show an airliner in the colors of a fictional airline, but instead used a 747 painted in the original Boeing livery. Very bad. The plot is stupid and has been done many times before, only much, much better. There are so many ridiculous moments here that i lost count of it really early. Also, I was on the bad guys\\' side all the time in the movie, because the good guys were so stupid. \"Executive Decision\" should without a doubt be you\\'re choice over this one, even the \"Turbulence\"-movies are better. In fact, every other movie in the world is better than this one.'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files(\"data/aclImdb/train/\")\n",
    "\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "\n",
    "print(\"tipe data text_train: {}\".format(type(text_train)))\n",
    "print(\"panjang data text_train: {}\".format(len(text_train)))\n",
    "print(\"data pertama text_train: {}\".format(text_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumlah sample tiap kelas untuk data training: [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"jumlah sample tiap kelas untuk data training: {}\".format(np.bincount(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tipe data text_test: <class 'list'>\n",
      "panjang data text_test: 25000\n",
      "jumlah sample tiap kelas untuk data testing: [12500 12500]\n",
      "data pertama text_test: b'I don\\'t know how this movie has received so many positive comments. One can call it \"artistic\" and \"beautifully filmed\", but those things don\\'t make up for the empty plot that was filled with sexual innuendos. I wish I had not wasted my time to watch this movie. Rather than being biographical, it was a poor excuse for promoting strange and lewd behavior. It was just another Hollywood attempt to convince us that that kind of life is normal and OK. From the very beginning I asked my self what was the point of this movie,and I continued watching, hoping that it would change and was quite disappointed that it continued in the same vein. I am so glad I did not spend the money to see this in a theater!'\n"
     ]
    }
   ],
   "source": [
    "reviews_test = load_files(\"data/aclImdb/test/\")\n",
    "\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "\n",
    "print(\"tipe data text_test: {}\".format(type(text_test)))\n",
    "print(\"panjang data text_test: {}\".format(len(text_test)))\n",
    "print(\"jumlah sample tiap kelas untuk data testing: {}\".format(np.bincount(y_test)))\n",
    "print(\"data pertama text_test: {}\".format(text_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "REPLACE_TANPA_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "REPLACE_DENGAN_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    reviews = [REPLACE_TANPA_SPACE.sub(\"\", line.decode('utf-8').lower()) for line in reviews]\n",
    "    reviews = [REPLACE_DENGAN_SPACE.sub(\" \", line) for line in reviews]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "text_train = preprocess_reviews(text_train)\n",
    "text_test = preprocess_reviews(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero day leads you to think even re think why two boys young men would do what they did   commit mutual suicide via slaughtering their classmates it captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own mutual world via coupled destruction it is not a perfect movie but given what money time the filmmaker and actors had   it is a remarkable product in terms of explaining the motives and actions of the two young suicide murderers it is better than elephant   in terms of being a film that gets under our rationalistic skin it is a far far better film than almost anything you are likely to see  flawed but honest with a terrible honesty\n"
     ]
    }
   ],
   "source": [
    "print(text_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membentuk Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "baw_words = [\"Saya menyukai bermain sepakbola di lapangan sepakbola\",\n",
    "             \"Budi bermain sepakbola bersama teman-teman di lapangan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenisasi\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vect.fit(baw_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "{'saya': 6, 'menyukai': 5, 'bermain': 0, 'sepakbola': 7, 'di': 3, 'lapangan': 4, 'budi': 2, 'bersama': 1, 'teman': 8}\n"
     ]
    }
   ],
   "source": [
    "print(len(vect.vocabulary_))\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<2x9 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 13 stored elements in Compressed Sparse Row format>\n",
      "  (0, 0)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t2\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t2\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = vect.transform(baw_words)\n",
    "print(repr(bag_of_words))\n",
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 1 1 1 2 0]\n",
      " [1 1 1 1 1 0 0 1 2]]\n"
     ]
    }
   ],
   "source": [
    "# merubah menjadi array\n",
    "print(bag_of_words.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW data Imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<25000x92715 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3461902 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(repr(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92715\n",
      "['00', '000', '0000000000001', '000001', '0001', '00015', '001', '002', '003830', '006', '007', '0079', '0080', '0083', '00s', '01', '010', '0130', '02', '020410html']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "print(len(feature_names))\n",
    "print(feature_names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88688\n",
      "{'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# uji klasifikasi dengan logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87952\n"
     ]
    }
   ],
   "source": [
    "X_test = vect.transform(text_test)\n",
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengatasi data/fitur yang tidak berguna\n",
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<25000x27994 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3364141 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "print(repr(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27994\n",
      "['00', '007', '00s', '01', '02', '05', '06', '07', '08', '09', '10', '100', '1000', '10000', '100000', '1000000', '100th', '101', '102', '103']\n",
      "['rasuk', 'rat', 'ratchet', 'rate', 'rated', 'rates', 'rathbone', 'rather', 'rating', 'ratings', 'ratio']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "print(len(feature_names))\n",
    "print(feature_names[:20])\n",
    "print(feature_names[20000:20011])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88648\n"
     ]
    }
   ],
   "source": [
    "# retrain\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menghapus Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n",
      "['though', 'your', 'cry', 'via', 'hereupon', 'ltd', 'nothing', 'whereupon', 'five', 'across']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "print(len(ENGLISH_STOP_WORDS))\n",
    "print(list(ENGLISH_STOP_WORDS)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<25000x27688 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 2161888 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# tambah stop_words pada CountVectorizer\n",
    "vect = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "print(repr(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88152\n"
     ]
    }
   ],
   "source": [
    "# gunakan GridSearchCV\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescaling Data dengan tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8938\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None), LogisticRegression())\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(text_train, y_train)\n",
    "\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitur dengan tf-idf terkecil:\n",
      "['poignant' 'disagree' 'instantly' 'importantly' 'lacked' 'currently'\n",
      " 'occurred' 'altogether' 'nearby' 'undoubtedly' 'fond' 'directs' 'avoided'\n",
      " 'emphasis' 'commented' 'stinker' 'disappoint' 'realizing' 'downhill'\n",
      " 'inane']\n",
      "Fitur dengan tf-idf terbesar:\n",
      "['kornbluth' 'europa' 'ripley' 'roy' 'blob' 'gadget' 'dillinger'\n",
      " 'hackenstein' 'basket' 'homer' 'dominick' 'bridget' 'taker' 'vargas'\n",
      " 'jesse' 'victor' 'timon' 'the' 'rob' 'titanic']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = grid.best_estimator_.named_steps['tfidfvectorizer']\n",
    "\n",
    "# transform data traininig\n",
    "X_train = vectorizer.transform(text_train)\n",
    "\n",
    "# cari nilai maximum dari fitur\n",
    "max_value = X_train.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "\n",
    "# nama fitur\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "print(\"Fitur dengan tf-idf terkecil:\")\n",
    "print(feature_names[sorted_by_tfidf[:20]])\n",
    "\n",
    "print(\"Fitur dengan tf-idf terbesar:\")\n",
    "print(feature_names[sorted_by_tfidf[-20:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitur dengan idf terkecil:\n",
      "['the' 'and' 'of' 'to' 'this' 'is' 'in' 'it' 'that' 'for' 'but' 'with'\n",
      " 'was' 'as' 'on' 'movie' 'not' 'have' 'be' 'one' 'are' 'film' 'all' 'at'\n",
      " 'you' 'its' 'an' 'by' 'from' 'so' 'like' 'who' 'his' 'if' 'out' 'just'\n",
      " 'about' 'they' 'or' 'has' 'he' 'there' 'some' 'good' 'what' 'more' 'when'\n",
      " 'time' 'very' 'up' 'even' 'only' 'no' 'my' 'see' 'would' 'can' 'really'\n",
      " 'story' 'which' 'had' 'well' 'me' 'than' 'much' 'were' 'their' 'get'\n",
      " 'other' 'been' 'do' 'most' 'her' 'also' 'into' 'first' 'made' 'dont'\n",
      " 'great' 'how' 'will' 'because' 'make' 'people' 'way' 'bad' 'could' 'any'\n",
      " 'after' 'too' 'then' 'movies' 'them' 'we' 'watch' 'think' 'acting' 'seen'\n",
      " 'characters' 'she']\n"
     ]
    }
   ],
   "source": [
    "sorted_by_idf = np.argsort(vectorizer.idf_)\n",
    "print(\"Fitur dengan idf terkecil:\")\n",
    "print(feature_names[sorted_by_idf[:100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88508"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score predict\n",
    "grid.score(text_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
