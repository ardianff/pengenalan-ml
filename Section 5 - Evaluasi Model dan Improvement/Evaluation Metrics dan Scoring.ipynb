{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics untuk Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imbalanced datasets\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "y = digits.target == 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, y, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_majority = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\n",
    "pred_most_frequent = dummy_majority.predict(X_test)\n",
    "\n",
    "print(\"Test score: {:.2f}\".format(dummy_majority.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=2).fit(X_train, y_train)\n",
    "pred_tree = tree.predict(X_test)\n",
    "\n",
    "print(\"Test score: {:.2f}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(C=0.1).fit(X_train, y_train)\n",
    "pred_logreg = logreg.predict(X_test)\n",
    "\n",
    "print(\"Test score: {:.2f}\".format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[408   5]\n",
      " [  7  30]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion = confusion_matrix(y_test, pred_logreg)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred_logreg).ravel()\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn: 408\n",
      "fp: 5\n",
      "fn: 7\n",
      "tp: 30\n"
     ]
    }
   ],
   "source": [
    "print(\"tn: {}\".format(tn))\n",
    "print(\"fp: {}\".format(fp))\n",
    "print(\"fn: {}\".format(fn))\n",
    "print(\"tp: {}\".format(tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dummy model:\n",
      "[[413   0]\n",
      " [ 37   0]]\n",
      "\n",
      "Decision tree:\n",
      "[[399  14]\n",
      " [ 21  16]]\n",
      "\n",
      "Logistic Regression:\n",
      "[[408   5]\n",
      " [  7  30]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDummy model:\")\n",
    "print(confusion_matrix(y_test, pred_most_frequent))\n",
    "print(\"\\nDecision tree:\")\n",
    "print(confusion_matrix(y_test, pred_tree))\n",
    "print(\"\\nLogistic Regression:\")\n",
    "print(confusion_matrix(y_test, pred_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score dummy: 0.00\n",
      "f1 score decision tree: 0.48\n",
      "f1 score logistic regression: 0.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(\"f1 score dummy: {:.2f}\".format(f1_score(y_test, pred_most_frequent)))\n",
    "print(\"f1 score decision tree: {:.2f}\".format(f1_score(y_test, pred_tree)))\n",
    "print(\"f1 score logistic regression: {:.2f}\".format(f1_score(y_test, pred_logreg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   not nine       0.92      1.00      0.96       413\n",
      "       nine       0.00      0.00      0.00        37\n",
      "\n",
      "avg / total       0.84      0.92      0.88       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, pred_most_frequent, target_names=[\"not nine\", \"nine\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   not nine       0.95      0.97      0.96       413\n",
      "       nine       0.53      0.43      0.48        37\n",
      "\n",
      "avg / total       0.92      0.92      0.92       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_tree, target_names=[\"not nine\", \"nine\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   not nine       0.98      0.99      0.99       413\n",
      "       nine       0.86      0.81      0.83        37\n",
      "\n",
      "avg / total       0.97      0.97      0.97       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_logreg, target_names=[\"not nine\", \"nine\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics untuk Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.951\n",
      "Confusion matrix:\n",
      "[[43  0  0  0  0  0  0  1  0  0]\n",
      " [ 0 44  0  1  0  0  1  0  4  1]\n",
      " [ 0  0 48  1  0  0  0  0  0  0]\n",
      " [ 0  0  0 34  0  0  0  0  2  1]\n",
      " [ 0  0  0  0 49  0  0  0  0  1]\n",
      " [ 0  1  0  0  0 45  0  0  0  1]\n",
      " [ 0  0  0  0  0  0 53  0  1  0]\n",
      " [ 0  0  0  0  0  0  0 41  0  0]\n",
      " [ 0  2  0  1  0  0  0  0 37  0]\n",
      " [ 0  0  0  1  0  0  0  0  2 34]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, random_state=30)\n",
    "\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "pred = lr.predict(X_test)\n",
    "\n",
    "print(\"Accuracy score: {:.3f}\".format(accuracy_score(y_test, pred)))\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.98      0.99        44\n",
      "          1       0.94      0.86      0.90        51\n",
      "          2       1.00      0.98      0.99        49\n",
      "          3       0.89      0.92      0.91        37\n",
      "          4       1.00      0.98      0.99        50\n",
      "          5       1.00      0.96      0.98        47\n",
      "          6       0.98      0.98      0.98        54\n",
      "          7       0.98      1.00      0.99        41\n",
      "          8       0.80      0.93      0.86        40\n",
      "          9       0.89      0.92      0.91        37\n",
      "\n",
      "avg / total       0.95      0.95      0.95       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro average f1 score: 0.951\n",
      "Macro average f1 score: 0.949\n"
     ]
    }
   ],
   "source": [
    "# f1-score\n",
    "# micro: menghitung rata-rata dari jumlah FP, FN, TP pada semua kelas\n",
    "print(\"Micro average f1 score: {:.3f}\".format(f1_score(y_test, pred, average=\"micro\")))\n",
    "\n",
    "# macro: menghitung rata-rata dari tiap kelas yang tidak berbobot  \n",
    "print(\"Macro average f1 score: {:.3f}\".format(f1_score(y_test, pred, average=\"macro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score: 0.947\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_true = [0.5, 2.1, 3.5]\n",
    "y_pred = [0.7, 1.9, 3.1]\n",
    "\n",
    "print(\"r2 score: {:.3f}\".format(r2_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score: 1.000\n"
     ]
    }
   ],
   "source": [
    "y_true = [1, 2, 3]\n",
    "y_pred = [1, 2, 3]\n",
    "\n",
    "print(\"r2 score: {:.3f}\".format(r2_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score: -1.357\n"
     ]
    }
   ],
   "source": [
    "y_true = [4, 2, 1]\n",
    "y_pred = [3, 1, 4]\n",
    "\n",
    "print(\"r2 score: {:.3f}\".format(r2_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
